{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94195e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import random\n",
    "import time\n",
    "\n",
    "openai.api_key = \"API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "18f92259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(df_2):\n",
    "    #remove URL\n",
    "    df_2['tweet_proc'] = df_2['Tweet'].str.replace(r'http(\\S)+', r'')\n",
    "    df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'http ...', r'')\n",
    "    df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'http', r'')\n",
    "    df_2[df_2['tweet_proc'].str.contains(r'http')]\n",
    "\n",
    "    # remove RT, @\n",
    "    df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "    df_2[df_2['tweet_proc'].str.contains(r'RT[ ]?@')]\n",
    "    df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'@[\\S]+',r'')\n",
    "\n",
    "    #remove non-ascii words and characters\n",
    "    df_2['tweet_proc'] = [''.join([i if ord(i) < 128 else '' for i in text]) for text in df_2['tweet_proc']]\n",
    "    df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'_[\\S]?',r'')\n",
    "\n",
    "    #remove &, < and >\n",
    "    df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'&amp;?',r'and')\n",
    "    df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'&lt;',r'<')\n",
    "    df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'&gt;',r'>')\n",
    "\n",
    "    # remove extra space\n",
    "    df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'[ ]{2, }',r' ')\n",
    "\n",
    "    # insert space between punctuation marks\n",
    "    df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "    df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "\n",
    "    # lower case and strip white spaces at both ends\n",
    "    df_2['tweet_proc'] = df_2['tweet_proc'].str.lower()\n",
    "    df_2['tweet_proc'] = df_2['tweet_proc'].str.strip()\n",
    "    #We will calculate the length of each tweet and only keep unique tweets that are 3 words or longer.\n",
    "\n",
    "    df_2['tweet_proc_length'] = [len(text.split(' ')) for text in df_2['tweet_proc']]\n",
    "    df_2['tweet_proc_length'].value_counts()\n",
    "\n",
    "    df_2 = df_2[df_2['tweet_proc_length']>3]\n",
    "    df_2 = df_2.drop_duplicates(subset=['tweet_proc'])\n",
    "    tweets_list = df_2['tweet_proc']\n",
    "    return tweets_list\n",
    "\n",
    "# Extract informations and tweets of each Account to use in prompts \n",
    "def information(df, username):\n",
    "    df = df[df['conversationId'] == df['id']]\n",
    "    df = df.reset_index(drop=True)\n",
    "    tweets = text_cleaning(df)\n",
    "    tweets_shuffled = tweets.sample(frac=1)\n",
    "    tweets_shuffled = tweets_shuffled.reset_index(drop=True)\n",
    "\n",
    "    df_accounts = pd.read_csv(\"accounts.csv\")\n",
    "    df_accounts = df_accounts[df_accounts['username'] == username]\n",
    "    df_accounts = df_accounts.reset_index(drop=True)\n",
    "    name = df_accounts['display name'][0]\n",
    "    followers = df_accounts['followers count'][0]\n",
    "    friends = df_accounts['friends count'][0]\n",
    "    page_age = df_accounts['user created date'][0][0:4]\n",
    "    verified = df_accounts['is verified'][0]\n",
    "    return tweets_shuffled, name, followers, friends, page_age, verified\n",
    "\n",
    "# Function to send a message to the OpenAI chatbot model and return its response\n",
    "def send_message(message_log):\n",
    "    # Use OpenAI's ChatCompletion API to get the chatbot's response\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",  # The name of the OpenAI chatbot model to use\n",
    "        messages=message_log,   # The conversation history up to this point, as a list of dictionaries\n",
    "        max_tokens=3800,        # The maximum number of tokens (words or subwords) in the generated response\n",
    "        stop=None,              # The stopping sequence for the generated response, if any (not used here)\n",
    "        temperature=0.7,        # The \"creativity\" of the generated response (higher temperature = more creative)\n",
    "    )\n",
    "\n",
    "    # Find the first response from the chatbot that has text in it (some responses may not have text)\n",
    "    for choice in response.choices:\n",
    "        if \"text\" in choice:\n",
    "            return choice.text\n",
    "\n",
    "    # If no response with text is found, return the first response's content (which may be empty)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def first_prompt(tweets, name, followers, friends, page_age, verified):\n",
    "    len_tweets = len(tweets)\n",
    "    prompts_list = []\n",
    "    for i in range(20):\n",
    "        random_list = []\n",
    "        # Set a length of the list to 10\n",
    "        for i in range(0, 4):\n",
    "            # any random numbers from 0 to 1000\n",
    "            random_list.append(random.randint(0, len_tweets-1))\n",
    "        prompt = \"\"\"write exactly two paragraph description for username {}: {} is a {} verified user in tweeter since {} and she has {} followers and {} friends,\n",
    "she posts following tweets in her timeline:\n",
    "t1 = {}\n",
    "t2 = {}\n",
    "t3 = {}\n",
    "t4 = {}\n",
    "\"\"\".format(name, name, verified, page_age, followers, friends, tweets[random_list[0]], tweets[random_list[1]], tweets[random_list[2]], tweets[random_list[3]])\n",
    "        prompts_list.append(prompt)\n",
    "    return prompts_list\n",
    "\n",
    "def second_prompt(paragraph_list, mixed_numbers):\n",
    "    len_paragraph = len(paragraph_list)\n",
    "    prompts_list = []\n",
    "    for i in range(mixed_numbers):\n",
    "        random_list = []\n",
    "        # Set a length of the list to 10\n",
    "        for i in range(0, 2):\n",
    "            # any random numbers from 0 to 1000\n",
    "            random_list.append(random.randint(0, len_paragraph-1))\n",
    "        prompt = \"\"\"rewrite it more informative but summarized in human words without extra words:\n",
    "s1 = {}\n",
    "s2 = {}\n",
    "\"\"\".format(paragraph_list[random_list[0]], paragraph_list[random_list[1]])\n",
    "        prompts_list.append(prompt)\n",
    "    return prompts_list\n",
    "\n",
    "def paragraphs(prompts_list):\n",
    "    # Initialize the conversation history with a message from the chatbot\n",
    "    message_log = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "    summaries_list = []\n",
    "    # Set a flag to keep track how many summaries we got\n",
    "    summaries = 0\n",
    "\n",
    "    # Start a loop that runs until prompts finished\n",
    "    for prompts in prompts_list:\n",
    "        message_log = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "    ]\n",
    "        # if the len of the prompt is not higher than 1100 character we can make the summary because of tokens limitation\n",
    "        if len(prompts) > 1100:\n",
    "            print('pass')\n",
    "            time.sleep(10)\n",
    "            pass\n",
    "        else :\n",
    "            user_input = prompts\n",
    "            print(len(prompts))\n",
    "            message_log.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "            # Add a message from the chatbot to the conversation history\n",
    "            message_log.append({\"role\": \"assistant\", \"content\": \"You are a helpful assistant.\"})\n",
    "\n",
    "            # Send the conversation history to the chatbot and get its response\n",
    "            response = send_message(message_log)\n",
    "\n",
    "            # Add the chatbot's response to the conversation history and print it to the console\n",
    "            message_log.append({\"role\": \"assistant\", \"content\": response})\n",
    "            summaries_list.append(response)\n",
    "            summaries += 1\n",
    "            # Adding sleep timer to prevend request limitation of API\n",
    "            time.sleep(10)\n",
    "    return summaries_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce321bcf",
   "metadata": {},
   "source": [
    "# Ylecun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d1f3b29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25815/2784696263.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['Tweet'].str.replace(r'http(\\S)+', r'')\n",
      "/tmp/ipykernel_25815/2784696263.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'http ...', r'')\n",
      "/tmp/ipykernel_25815/2784696263.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
      "/tmp/ipykernel_25815/2784696263.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'@[\\S]+',r'')\n",
      "/tmp/ipykernel_25815/2784696263.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'_[\\S]?',r'')\n",
      "/tmp/ipykernel_25815/2784696263.py:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'&amp;?',r'and')\n",
      "/tmp/ipykernel_25815/2784696263.py:24: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'[ ]{2, }',r' ')\n",
      "/tmp/ipykernel_25815/2784696263.py:27: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
      "/tmp/ipykernel_25815/2784696263.py:28: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"ylecun_owner_all.csv\")\n",
    "tweets, name, followers, friends, page_age, verified = information(df, 'ylecun')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04122f3",
   "metadata": {},
   "source": [
    "First we create 20 prompts by mixing 4 random tweets to make initialized summaries and Improve in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a39a3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_list = first_prompt(tweets, name, followers, friends, page_age, verified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98cf520e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write exactly two paragraph description for username Yann LeCun: Yann LeCun is a False verified user in tweeter since 2009 and she has 434596 followers and 601 friends,\\nshe posts following tweets in her timeline:\\nt1 = scientific debates on social media are like a human form of bidirectional rlhf .\\n the person making the post gets feedback ( good and bad ).\\n the commenters also get feedback , mostly when they are clueless or wrong .\\nt2 = convnets are a decent model of how the ventral pathway of the human visual cortex works .\\n but llms don \\' t seem to be a good model of how humans process language .\\n there longer - term prediction taking place in the brain .\\n awesome work by the brain - ai group at fair - paris .\\nt3 = if using lisp as a front - end language to a deep learning system is \" neuro - symbolic \" then , i \\' ve doing neuro - symbolic stuff since 1987\\nt4 = from the horse \\' s mouth .\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_list[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fec77095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1101"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompts_list[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b548f",
   "metadata": {},
   "source": [
    "getting Initialized summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad22fc7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1057\n",
      "826\n",
      "840\n",
      "pass\n",
      "796\n",
      "pass\n",
      "887\n",
      "772\n",
      "pass\n",
      "942\n",
      "690\n",
      "pass\n",
      "903\n",
      "693\n",
      "1099\n",
      "pass\n",
      "1038\n",
      "1065\n",
      "984\n",
      "pass\n"
     ]
    }
   ],
   "source": [
    "summaries_list = paragraphs(prompts_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc342b3",
   "metadata": {},
   "source": [
    "Seperating Paragraph 1 and 2 to have 2 seperate paragraphs and handling OpenAI tokens limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca7686a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_1_list = []\n",
    "paragraph_2_list = []\n",
    "for text in summaries_list:\n",
    "    x = text.split(\"\\n\\n\")\n",
    "    paragraph_1 = x[0]\n",
    "    paragraph_2 = x[1]\n",
    "    paragraph_1_list.append(paragraph_1)\n",
    "    paragraph_2_list.append(paragraph_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331aa075",
   "metadata": {},
   "source": [
    "creating 10 prompts for each paragraph by mixing 2 random summarized texts we got from last part \n",
    "by doing this we got more detailed and robust summaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0997795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n",
      "1030\n",
      "940\n",
      "980\n",
      "pass\n",
      "1011\n",
      "1011\n",
      "952\n",
      "pass\n",
      "963\n"
     ]
    }
   ],
   "source": [
    "paragraph1 = second_prompt(paragraph_1_list, 10)\n",
    "summaries_list_p1 = paragraphs(paragraph1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae56d581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n",
      "pass\n",
      "1056\n",
      "1038\n",
      "1047\n",
      "1038\n",
      "pass\n",
      "1047\n",
      "1001\n",
      "996\n"
     ]
    }
   ],
   "source": [
    "paragraph2 = second_prompt(paragraph_2_list, 10)\n",
    "summaries_list_p2 = paragraphs(paragraph2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea55a79",
   "metadata": {},
   "source": [
    "now We get the smallest summaries for each paragraph, smallest one has less miss informations and AI irrelevant texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eae7f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_min(paragraph_list):\n",
    "    p = []\n",
    "    min_len = 10000\n",
    "    for s in paragraph_list:\n",
    "        if len(s) < min_len:\n",
    "            min_len = len(s)\n",
    "            p = s \n",
    "        else:\n",
    "            continue\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c80c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = p_min(summaries_list_p1)\n",
    "p2 = p_min(summaries_list_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e0e02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "promt = [\"\"\" \n",
    "write it as human two paragraph writing without any extra words:\n",
    "{}\n",
    "{}\n",
    "\"\"\".format(p1, p2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac1ef6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770\n"
     ]
    }
   ],
   "source": [
    "final = paragraphs(promt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8de50078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Yann LeCun is a renowned computer scientist and the Director of AI Research at Facebook. He is an expert in the field of artificial intelligence and regularly shares his knowledge with his 434,596 Twitter followers and 601 friends. Yann's tweets cover a range of topics related to AI, including machine learning and deep learning, and he keeps his followers up-to-date with the latest trends and developments in the field.\\n\\nYann is known for his candid opinions and has a large following on Twitter. He regularly hosts Q&A sessions and invites questions on AI and creativity. Yann's keynote speeches at conferences are highly regarded, and his technical insights and humorous observations on Twitter are closely followed by others in the AI community. Overall, Yann LeCun is a respected figure in the field of AI, and his contributions to the community are highly valued.\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c28653",
   "metadata": {},
   "source": [
    "# Taylor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52cb29cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25815/326855171.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['Tweet'].str.replace(r'http(\\S)+', r'')\n",
      "/tmp/ipykernel_25815/326855171.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'http ...', r'')\n",
      "/tmp/ipykernel_25815/326855171.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
      "/tmp/ipykernel_25815/326855171.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'@[\\S]+',r'')\n",
      "/tmp/ipykernel_25815/326855171.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'_[\\S]?',r'')\n",
      "/tmp/ipykernel_25815/326855171.py:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'&amp;?',r'and')\n",
      "/tmp/ipykernel_25815/326855171.py:24: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'[ ]{2, }',r' ')\n",
      "/tmp/ipykernel_25815/326855171.py:27: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
      "/tmp/ipykernel_25815/326855171.py:28: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"taylor_owner_all.csv\")\n",
    "tweets, name, followers, friends, page_age, verified = information(df, 'TaylorLorenz')\n",
    "prompts_list = first_prompt(tweets, name, followers, friends, page_age, verified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "385df250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "655\n",
      "919\n",
      "896\n",
      "688\n",
      "854\n",
      "666\n",
      "865\n",
      "964\n",
      "673\n",
      "415\n",
      "909\n",
      "579\n",
      "684\n",
      "880\n",
      "821\n",
      "693\n",
      "813\n",
      "572\n",
      "628\n",
      "545\n"
     ]
    }
   ],
   "source": [
    "summaries_list = paragraphs(prompts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99bfff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_1_list = []\n",
    "paragraph_2_list = []\n",
    "for text in summaries_list:\n",
    "    x = text.split(\"\\n\\n\")\n",
    "    paragraph_1 = x[0]\n",
    "    paragraph_2 = x[1]\n",
    "    paragraph_1_list.append(paragraph_1)\n",
    "    paragraph_2_list.append(paragraph_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c7c3b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "957\n",
      "735\n",
      "943\n",
      "918\n",
      "685\n",
      "1001\n",
      "816\n",
      "816\n",
      "882\n",
      "pass\n",
      "1052\n",
      "1053\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "1026\n",
      "1030\n",
      "pass\n"
     ]
    }
   ],
   "source": [
    "paragraph1 = second_prompt(paragraph_1_list, 10)\n",
    "summaries_list_p1 = paragraphs(paragraph1)\n",
    "paragraph2 = second_prompt(paragraph_2_list, 10)\n",
    "summaries_list_p2 = paragraphs(paragraph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c1bd555",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = p_min(summaries_list_p1)\n",
    "p2 = p_min(summaries_list_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3bba9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Taylor Lorenz is a journalist who is well known for her writing on social media trends and the impact of technology on culture. She has a large Twitter following of over 350,000 users and has been a verified user since 2010. Lorenz currently works as a staff writer at The New York Times.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49f8c0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lorenz's tweets reflect her unique perspective and diverse interests. She's not afraid to discuss controversial topics or express her opinions on important issues. Her tweets are engaging and thought-provoking, whether she's talking about fashion or calling out unethical behavior in the media. Her followers appreciate her honesty and authenticity, and she's gained a reputation as a trusted voice in social media and journalism. Both sentences are the same.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a99bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "promt = [\"\"\" \n",
    "write it as human exactly two paragraphs /n/n writing without any extra words:\n",
    "{}\n",
    "{}\n",
    "\"\"\".format(p1, p2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "78774155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830\n"
     ]
    }
   ],
   "source": [
    "final_taylor = paragraphs(promt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "337d42ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Taylor Lorenz is a journalist whose writing focuses on social media trends and the impact of technology on culture. She has a verified Twitter account with over 350,000 followers, and she has been active on the platform since 2010. Currently, Lorenz works as a staff writer for The New York Times.\\n\\nLorenz's tweets are engaging and cover a wide range of topics, from fashion to ethics in media. She is not afraid to express her opinions on controversial issues, and her followers appreciate her authenticity and honesty. Her unique perspective and thought-provoking tweets have earned her a reputation as a trusted voice in both social media and journalism.\"]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_taylor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e300a0ae",
   "metadata": {},
   "source": [
    "# Cathie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4f78ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25815/1995425633.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['Tweet'].str.replace(r'http(\\S)+', r'')\n",
      "/tmp/ipykernel_25815/1995425633.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'http ...', r'')\n",
      "/tmp/ipykernel_25815/1995425633.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
      "/tmp/ipykernel_25815/1995425633.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'@[\\S]+',r'')\n",
      "/tmp/ipykernel_25815/1995425633.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'_[\\S]?',r'')\n",
      "/tmp/ipykernel_25815/1995425633.py:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'&amp;?',r'and')\n",
      "/tmp/ipykernel_25815/1995425633.py:24: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'[ ]{2, }',r' ')\n",
      "/tmp/ipykernel_25815/1995425633.py:27: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
      "/tmp/ipykernel_25815/1995425633.py:28: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['tweet_proc'] = df_2['tweet_proc'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cathiedwood_owner_all.csv\")\n",
    "tweets, name, followers, friends, page_age, verified = information(df, 'CathieDWood')\n",
    "prompts_list = first_prompt(tweets, name, followers, friends, page_age, verified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8fae0401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1070\n",
      "1135\n",
      "pass\n",
      "1079\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "1113\n",
      "1149\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "947\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "996\n",
      "pass\n",
      "pass\n"
     ]
    }
   ],
   "source": [
    "summaries_list = paragraphs(prompts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4ae93fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_1_list = []\n",
    "paragraph_2_list = []\n",
    "for text in summaries_list:\n",
    "    x = text.split(\"\\n\\n\")\n",
    "    paragraph_1 = x[0]\n",
    "    paragraph_2 = x[1]\n",
    "    paragraph_1_list.append(paragraph_1)\n",
    "    paragraph_2_list.append(paragraph_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "848bf3a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "959\n",
      "1041\n",
      "1041\n",
      "pass\n",
      "1019\n",
      "pass\n",
      "1027\n",
      "1019\n",
      "1027\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n"
     ]
    }
   ],
   "source": [
    "paragraph1 = second_prompt(paragraph_1_list, 10)\n",
    "summaries_list_p1 = paragraphs(paragraph1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "307d335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1237\n",
      "1127\n",
      "1288\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "1127\n"
     ]
    }
   ],
   "source": [
    "paragraph2 = second_prompt(paragraph_2_list, 10)\n",
    "summaries_list_p2 = paragraphs(paragraph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d179dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = p_min(summaries_list_p1)\n",
    "p2 = p_min(summaries_list_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0c054976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cathie Wood is a famous finance and investment expert. She created a company called ARK Invest that invests in companies that lead in technology innovation. She has a lot of followers on Twitter, where she shares her investment strategies and market insights. People look up to her for guidance on how to invest in the financial market.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8683f60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cathie Wood shares her knowledge on finance and technology trends via Twitter. She focuses on emerging technologies with the potential to transform industries, such as digital TV advertising. She also discusses the impact of innovation on economic theories like Keynesian economics. Her large Twitter following makes her a valuable source of information for investors and industry professionals.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "88f86fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "promt = [\"\"\" \n",
    "write it as human exactly two paragraphs /n/n writing without any extra words:\n",
    "{}\n",
    "{}\n",
    "\"\"\".format(p1, p2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "61a9d15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814\n"
     ]
    }
   ],
   "source": [
    "final_cathie = paragraphs(promt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6d9b645c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cathie Wood is a well-known finance and investment expert. She is the founder of ARK Invest, a company that invests in innovative technology companies. Cathie has a significant following on Twitter, where she shares her investment strategies and market insights. Her followers look up to her for advice on how to invest in the financial market, especially in emerging technologies that can transform industries.\\n\\nThrough her Twitter account, Cathie shares her knowledge on finance and technology trends. She concentrates on emerging technologies that have the potential to revolutionize industries, like digital TV advertising. Additionally, she discusses the impact of innovation on economic theories such as Keynesian economics. Due to her large Twitter following, Cathie is a valuable source of information for investors and industry professionals alike.']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_cathie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "005045a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cathie = final_cathie[0]\n",
    "final = final[0]\n",
    "final_taylor = final_taylor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b31f313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Accounts': ['ylecun', 'TaylorLorenz', 'CathieDWood'], 'Description': [final, final_taylor, final_cathie]}\n",
    "df_descriptions = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e38a54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_descriptions.to_csv(\"Descriptions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fb3a2e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yann LeCun is a renowned computer scientist and the Director of AI Research at Facebook. He is an expert in the field of artificial intelligence and regularly shares his knowledge with his 434,596 Twitter followers and 601 friends. Yann's tweets cover a range of topics related to AI, including machine learning and deep learning, and he keeps his followers up-to-date with the latest trends and developments in the field.\n",
      "\n",
      "Yann is known for his candid opinions and has a large following on Twitter. He regularly hosts Q&A sessions and invites questions on AI and creativity. Yann's keynote speeches at conferences are highly regarded, and his technical insights and humorous observations on Twitter are closely followed by others in the AI community. Overall, Yann LeCun is a respected figure in the field of AI, and his contributions to the community are highly valued.\n"
     ]
    }
   ],
   "source": [
    "print(df_descriptions['Description'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2d358b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taylor Lorenz is a journalist whose writing focuses on social media trends and the impact of technology on culture. She has a verified Twitter account with over 350,000 followers, and she has been active on the platform since 2010. Currently, Lorenz works as a staff writer for The New York Times.\n",
      "\n",
      "Lorenz's tweets are engaging and cover a wide range of topics, from fashion to ethics in media. She is not afraid to express her opinions on controversial issues, and her followers appreciate her authenticity and honesty. Her unique perspective and thought-provoking tweets have earned her a reputation as a trusted voice in both social media and journalism.\n"
     ]
    }
   ],
   "source": [
    "print(df_descriptions['Description'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "48ab5740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cathie Wood is a well-known finance and investment expert. She is the founder of ARK Invest, a company that invests in innovative technology companies. Cathie has a significant following on Twitter, where she shares her investment strategies and market insights. Her followers look up to her for advice on how to invest in the financial market, especially in emerging technologies that can transform industries.\n",
      "\n",
      "Through her Twitter account, Cathie shares her knowledge on finance and technology trends. She concentrates on emerging technologies that have the potential to revolutionize industries, like digital TV advertising. Additionally, she discusses the impact of innovation on economic theories such as Keynesian economics. Due to her large Twitter following, Cathie is a valuable source of information for investors and industry professionals alike.\n"
     ]
    }
   ],
   "source": [
    "print(df_descriptions['Description'][2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
