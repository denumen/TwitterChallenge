{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import clear_output\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-05T17:48:05.862485Z","iopub.execute_input":"2023-03-05T17:48:05.863803Z","iopub.status.idle":"2023-03-05T17:48:05.870478Z","shell.execute_reply.started":"2023-03-05T17:48:05.863749Z","shell.execute_reply":"2023-03-05T17:48:05.868774Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-03-05T16:34:21.332847Z","iopub.execute_input":"2023-03-05T16:34:21.333454Z","iopub.status.idle":"2023-03-05T16:34:32.421880Z","shell.execute_reply.started":"2023-03-05T16:34:21.333416Z","shell.execute_reply":"2023-03-05T16:34:32.420714Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.26.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfile = \"/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\"\ndf = pd.read_csv(file, encoding='ISO-8859-1', usecols=[0,5], header=None)\\\n        .sample(frac=0.3, random_state=42)\n\ndf.columns = ['label','sentence']\ndf.label = df.label.apply(lambda x: np.float64(1) if x==4 else np.float64(x))\n\nprint(\"df.shape =\",df.shape)\nprint(f\"label distribution :\\n{df.label.value_counts()}\")\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2023-03-05T17:46:16.496000Z","iopub.execute_input":"2023-03-05T17:46:16.496560Z","iopub.status.idle":"2023-03-05T17:46:23.405660Z","shell.execute_reply.started":"2023-03-05T17:46:16.496516Z","shell.execute_reply":"2023-03-05T17:46:23.403836Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"df.shape = (480000, 2)\nlabel distribution :\n1.0    240639\n0.0    239361\nName: label, dtype: int64\n        label                                           sentence\n541200    0.0             @chrishasboobs AHHH I HOPE YOUR OK!!! \n750       0.0  @misstoriblack cool , i have no tweet apps  fo...\n766711    0.0  @TiannaChaos i know  just family drama. its la...\n285055    0.0  School email won't open  and I have geography ...\n705995    0.0                             upper airways problem \n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, TFAutoModel\n\ncheckpoint = \"google/mobilebert-uncased\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = TFAutoModel.from_pretrained(checkpoint, output_hidden_states=True)\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2023-03-05T17:46:23.408313Z","iopub.execute_input":"2023-03-05T17:46:23.410793Z","iopub.status.idle":"2023-03-05T17:47:27.449577Z","shell.execute_reply.started":"2023-03-05T17:46:23.410728Z","shell.execute_reply":"2023-03-05T17:47:27.446627Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/847 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5637d36eb5a4b678cab1d5a7457f25a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5524ad751c7945e78b1fc1cafff83827"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d406e5cd4ae847eba1f76ef0b8efcfa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"tf_model.h5\";:   0%|          | 0.00/164M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81b0f8786eff4fb9bc1e4efdc6ca6686"}},"metadata":{}},{"name":"stderr","text":"Some layers from the model checkpoint at google/mobilebert-uncased were not used when initializing TFMobileBertModel: ['predictions___cls', 'seq_relationship___cls']\n- This IS expected if you are initializing TFMobileBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFMobileBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFMobileBertModel were initialized from the model checkpoint at google/mobilebert-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFMobileBertModel for predictions without further training.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/3802703648.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'clear_output' is not defined"],"ename":"NameError","evalue":"name 'clear_output' is not defined","output_type":"error"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\nsequences, test_val_sequences = train_test_split(df, test_size=0.3,\n                                             stratify=df.label, random_state=44)\nval_sequences, test_sequences = train_test_split(test_val_sequences, test_size=0.7,\n                                             stratify=test_val_sequences.label, random_state=44)\ndataset = {\n    \"TRAIN\": sequences['sentence'].values.tolist(),\n    \"TEST\": test_sequences['sentence'].values.tolist(),\n    \"VAL\": val_sequences['sentence'].values.tolist()\n}\ntargets = {\n    \"TRAIN\": sequences['label'].values.tolist(),\n    \"TEST\": test_sequences['label'].values.tolist(),\n    \"VAL\": val_sequences['label'].values.tolist()\n}","metadata":{"execution":{"iopub.status.busy":"2023-03-05T17:48:08.755788Z","iopub.execute_input":"2023-03-05T17:48:08.756261Z","iopub.status.idle":"2023-03-05T17:48:09.242631Z","shell.execute_reply.started":"2023-03-05T17:48:08.756223Z","shell.execute_reply":"2023-03-05T17:48:09.241047Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def tokenization(data, **kwargs):\n    return tokenizer(data, \n                   padding=kwargs.get('padding','longest'), \n                   max_length=kwargs.get('max_length',55),\n                   truncation=True, \n                   return_tensors=\"tf\")","metadata":{"execution":{"iopub.status.busy":"2023-03-05T17:48:09.884241Z","iopub.execute_input":"2023-03-05T17:48:09.885749Z","iopub.status.idle":"2023-03-05T17:48:09.895352Z","shell.execute_reply.started":"2023-03-05T17:48:09.885682Z","shell.execute_reply":"2023-03-05T17:48:09.893653Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_model(**kwargs):\n    global model\n    max_seq_length = kwargs.get('max_seq_length',55)\n\n    # Load tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained('google/mobilebert-uncased')\n    \n\n    input_ids = tf.keras.Input(shape=(max_seq_length,), dtype='int32', name='input_ids')\n    attention_mask = tf.keras.Input(shape=(max_seq_length,), dtype='int32', name='attention_mask')\n\n    # Tokenize inputs and pass them through the MobileBERT model\n    inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n    outputs = model(inputs)\n    pooler_output = outputs['pooler_output']\n\n    # Model Head\n    h1 = tf.keras.layers.Dense(128, activation='relu')(pooler_output)\n    dropout = tf.keras.layers.Dropout(0.2)(h1)\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)\n\n    # Create and compile the new model\n    new_model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n    metrics = [tf.keras.metrics.BinaryAccuracy()]\n    new_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n    return new_model","metadata":{"execution":{"iopub.status.busy":"2023-03-05T17:48:10.406915Z","iopub.execute_input":"2023-03-05T17:48:10.407401Z","iopub.status.idle":"2023-03-05T17:48:10.420017Z","shell.execute_reply.started":"2023-03-05T17:48:10.407339Z","shell.execute_reply":"2023-03-05T17:48:10.418254Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ndef test_result(model):    \n    test_inputs = tokenization(dataset[\"TEST\"])\n    result_proba = model.predict([test_inputs.input_ids, test_inputs.attention_mask])\n    result = [1 if x>0.5 else 0 for x in result_proba.ravel()]\n    print(classification_report(targets['TEST'],result))\n    return result_proba, result","metadata":{"execution":{"iopub.status.busy":"2023-03-05T17:48:10.833358Z","iopub.execute_input":"2023-03-05T17:48:10.834881Z","iopub.status.idle":"2023-03-05T17:48:10.855487Z","shell.execute_reply.started":"2023-03-05T17:48:10.834799Z","shell.execute_reply":"2023-03-05T17:48:10.847305Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"new_model = get_model()\n#result_proba_before, result_before = test_result(new_model)","metadata":{"execution":{"iopub.status.busy":"2023-03-05T17:48:11.880427Z","iopub.execute_input":"2023-03-05T17:48:11.882164Z","iopub.status.idle":"2023-03-05T17:48:33.590015Z","shell.execute_reply.started":"2023-03-05T17:48:11.882081Z","shell.execute_reply":"2023-03-05T17:48:33.588582Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"inputs = tokenization(dataset['TRAIN'])\ntrain_targets = tf.convert_to_tensor(targets['TRAIN'])\n\nval_inputs = tokenization(dataset['VAL'])\nval_targets = tf.convert_to_tensor(targets['VAL'])\n\n# Train the model\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', mode='max', patience=5)\n\nnew_model.fit([inputs.input_ids, inputs.attention_mask], train_targets, \n              validation_data = ([val_inputs.input_ids, val_inputs.attention_mask], val_targets),\n              epochs=100, batch_size=128, callbacks=[early_stop])","metadata":{"execution":{"iopub.status.busy":"2023-03-05T17:48:37.989746Z","iopub.execute_input":"2023-03-05T17:48:37.990636Z","iopub.status.idle":"2023-03-05T21:41:33.139954Z","shell.execute_reply.started":"2023-03-05T17:48:37.990591Z","shell.execute_reply":"2023-03-05T21:41:33.138444Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1/100\n2625/2625 [==============================] - 1272s 369ms/step - loss: 12513.8262 - binary_accuracy: 0.6402 - val_loss: 0.5832 - val_binary_accuracy: 0.6972\nEpoch 2/100\n2625/2625 [==============================] - 918s 350ms/step - loss: 0.5764 - binary_accuracy: 0.6976 - val_loss: 0.5410 - val_binary_accuracy: 0.7223\nEpoch 3/100\n2625/2625 [==============================] - 911s 347ms/step - loss: 0.5456 - binary_accuracy: 0.7227 - val_loss: 0.5157 - val_binary_accuracy: 0.7435\nEpoch 4/100\n2625/2625 [==============================] - 911s 347ms/step - loss: 0.5149 - binary_accuracy: 0.7461 - val_loss: 0.4843 - val_binary_accuracy: 0.7666\nEpoch 5/100\n2625/2625 [==============================] - 913s 348ms/step - loss: 0.4728 - binary_accuracy: 0.7760 - val_loss: 0.4421 - val_binary_accuracy: 0.7927\nEpoch 6/100\n2625/2625 [==============================] - 906s 345ms/step - loss: 0.4268 - binary_accuracy: 0.8044 - val_loss: 0.4070 - val_binary_accuracy: 0.8148\nEpoch 7/100\n2625/2625 [==============================] - 911s 347ms/step - loss: 0.3897 - binary_accuracy: 0.8258 - val_loss: 0.3779 - val_binary_accuracy: 0.8292\nEpoch 8/100\n2625/2625 [==============================] - 912s 347ms/step - loss: 0.3577 - binary_accuracy: 0.8426 - val_loss: 0.3562 - val_binary_accuracy: 0.8416\nEpoch 9/100\n2625/2625 [==============================] - 910s 347ms/step - loss: 0.3555 - binary_accuracy: 0.8462 - val_loss: 0.3576 - val_binary_accuracy: 0.8435\nEpoch 10/100\n2625/2625 [==============================] - 909s 346ms/step - loss: 0.3193 - binary_accuracy: 0.8630 - val_loss: 0.3465 - val_binary_accuracy: 0.8498\nEpoch 11/100\n2625/2625 [==============================] - 864s 329ms/step - loss: 0.3713 - binary_accuracy: 0.8701 - val_loss: 0.3569 - val_binary_accuracy: 0.8476\nEpoch 12/100\n2625/2625 [==============================] - 883s 336ms/step - loss: 0.2738 - binary_accuracy: 0.8854 - val_loss: 0.3563 - val_binary_accuracy: 0.8488\nEpoch 13/100\n2625/2625 [==============================] - 897s 342ms/step - loss: 0.2363 - binary_accuracy: 0.9033 - val_loss: 0.3799 - val_binary_accuracy: 0.8449\nEpoch 14/100\n2625/2625 [==============================] - 909s 346ms/step - loss: 0.2112 - binary_accuracy: 0.9144 - val_loss: 0.4226 - val_binary_accuracy: 0.8432\nEpoch 15/100\n2625/2625 [==============================] - 909s 346ms/step - loss: 0.1946 - binary_accuracy: 0.9218 - val_loss: 0.4931 - val_binary_accuracy: 0.8442\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f2b191fb390>"},"metadata":{}}]},{"cell_type":"code","source":"result_proba_after, result_after = test_result(new_model)","metadata":{"execution":{"iopub.status.busy":"2023-03-05T21:57:56.429409Z","iopub.execute_input":"2023-03-05T21:57:56.431142Z","iopub.status.idle":"2023-03-05T22:00:44.215228Z","shell.execute_reply.started":"2023-03-05T21:57:56.431057Z","shell.execute_reply":"2023-03-05T22:00:44.213737Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"3150/3150 [==============================] - 125s 36ms/step\n              precision    recall  f1-score   support\n\n         0.0       0.83      0.86      0.84     50266\n         1.0       0.85      0.83      0.84     50534\n\n    accuracy                           0.84    100800\n   macro avg       0.84      0.84      0.84    100800\nweighted avg       0.84      0.84      0.84    100800\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# SAVE MODEL WEIGHTS\nnew_model.save_weights(f'sentiment_weights_MobileBert_final.h5')\n!zip -r sentiment_weights_MobileBert_final.zip sentiment_weights_MobileBert_final.h5\nfrom IPython.display import FileLink\nFileLink(r'sentiment_weights_MobileBert_final.zip')","metadata":{"execution":{"iopub.status.busy":"2023-03-05T21:57:32.762796Z","iopub.execute_input":"2023-03-05T21:57:32.764256Z","iopub.status.idle":"2023-03-05T21:57:42.127769Z","shell.execute_reply.started":"2023-03-05T21:57:32.764182Z","shell.execute_reply":"2023-03-05T21:57:42.125972Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n  adding: sentiment_weights_MobileBert_final.h5 (deflated 8%)\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/sentiment_weights_MobileBert_final.zip","text/html":"<a href='sentiment_weights_MobileBert_final.zip' target='_blank'>sentiment_weights_MobileBert_final.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}